# Parallax ğŸš€

A research-grade JAX Transformer implementation from scratch, demonstrating the skills required for a Research Engineering role at labs like DeepMind or Google Research.

[![CI](https://github.com/ichbingautam/parallax/actions/workflows/ci.yml/badge.svg)](https://github.com/ichbingautam/parallax/actions/workflows/ci.yml)
[![Python](https://img.shields.io/badge/python-3.10+-blue)](https://www.python.org/)
[![JAX](https://img.shields.io/badge/JAX-0.4.30+-orange)](https://github.com/google/jax)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

---

## ğŸ¯ Project Overview

This project implements a **decoder-only Transformer language model** using modern techniques from state-of-the-art models like **Llama**, **Gemma**, and **PaLM**. It serves as a portfolio piece demonstrating:

- Deep understanding of Transformer architecture mathematics
- Production-quality JAX/Flax implementation
- Distributed training with `jax.pmap`
- Research engineering best practices

### Why This Project?

Research Engineering at labs like DeepMind differs from traditional software engineering:

| Standard SWE | Research Engineering |
|--------------|---------------------|
| Build features | Build experimental testbeds |
| Code is a product | Code is a scientific instrument |
| Ship fast | Ship correctly (numerical precision matters) |
| Unit tests | Overfit tests + gradient checks |

---

## ğŸ—ï¸ Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TransformerLM                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input IDs â†’ Embeddings â†’ [Block Ã— N] â†’ RMSNorm â†’ Logits   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TransformerBlock                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ RMSNorm â”‚â”€â”€â”€â–ºâ”‚ Multi-Head Attn   â”‚â”€â”€â”€â–ºâ”‚    +    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (RoPE + KV-Cache)â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚            â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                           â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ RMSNorm â”‚â”€â”€â”€â–ºâ”‚   SwiGLU FFN      â”‚â”€â”€â”€â–ºâ”‚    +    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚                                        â”‚            â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Techniques Explained

#### 1. RMSNorm (Root Mean Square Normalization)

**Used in**: Llama, Gemma, Mistral

Unlike LayerNorm which subtracts mean and divides by std, RMSNorm only divides by RMS:

```python
RMSNorm(x) = x / âˆš(mean(xÂ²) + Îµ) Ã— Î³
```

**Why?** Faster computation (no mean subtraction) with equal or better training stability.

#### 2. Rotary Positional Embeddings (RoPE)

**Used in**: Llama, GPT-NeoX, PaLM

RoPE encodes position by *rotating* query and key vectors:

```python
q_rotated = q Ã— cos(Î¸) + rotate_half(q) Ã— sin(Î¸)
```

**Why?**

- Relative position awareness through rotation
- Better extrapolation to longer sequences than absolute embeddings
- Position info naturally decays with distance

#### 3. SwiGLU Activation

**Used in**: Llama, PaLM, Gemma

```python
SwiGLU(x) = SiLU(xW_gate) âŠ™ (xW_up)  # âŠ™ = element-wise multiply
```

**Why?** Outperforms ReLU and GELU in practice (empirically shown in PaLM paper).

#### 4. Weight Tying

Shares weights between input embeddings and output projection:

```python
logits = hidden_states @ embedding_weights.T
```

**Why?** Reduces parameters by `vocab_size Ã— hidden_dim` (~16M for 32k vocab, 512 dim).

#### 5. Scaled Initialization

Output projections scaled by `1/âˆš(2N)` where N = num_layers:

```python
attn_out_proj.weight *= 1 / sqrt(2 * num_layers)
ffn_down_proj.weight *= 1 / sqrt(2 * num_layers)
```

**Why?** Prevents signal explosion in deep residual networks.

#### 6. Z-Loss (Logit Stability)

**Used in**: PaLM, Gemini

```python
z_loss = 1e-4 Ã— logÂ²(Î£ exp(logits))
```

**Why?** Prevents logits from drifting to extreme values, crucial for bfloat16 training on TPUs.

#### 7. KV-Cache

Caches key/value projections during autoregressive generation:

```
Step 1: Process "The" â†’ cache Kâ‚, Vâ‚
Step 2: Process "cat" â†’ cache Kâ‚‚, Vâ‚‚, attend to [Kâ‚,Kâ‚‚], [Vâ‚,Vâ‚‚]
Step 3: Process "sat" â†’ cache Kâ‚ƒ, Vâ‚ƒ, attend to [Kâ‚,Kâ‚‚,Kâ‚ƒ], [Vâ‚,Vâ‚‚,Vâ‚ƒ]
```

**Why?** Reduces generation complexity from O(NÂ²) to O(N).

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10+
- For GPU: CUDA 12.0+
- For TPU: Google Cloud account

### Local Development

```bash
# Clone repository
git clone https://github.com/ichbingautam/parallax.git
cd parallax

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install for CPU/GPU
pip install -e ".[dev]"

# Install for TPU
pip install -e ".[dev,tpu]"
```

---

## ğŸš€ Quick Start

### 1. Run the Overfit Test (Validate Correctness)

The **overfit test** is the most important validation for any new model implementation:

> If a model can't memorize 10 tokens, it won't learn 10 billion.

```bash
python scripts/train.py --mode overfit --steps 500
```

Expected output:

```
Final Loss: 0.0275
Accuracy: 100.0%
Target:     'B CD EF GH AB CD EF GH AB CD EF GH...'
Prediction: 'B CD EF GH AB CD EF GH AB CD EF GH...'

âœ… OVERFIT TEST PASSED - Model is mathematically correct!
```

### 2. Train on Tiny Shakespeare

```bash
python scripts/train.py --mode single --dataset tiny_shakespeare --steps 5000
```

### 3. Distributed Training (Multi-GPU/TPU)

```bash
python scripts/train.py --mode distributed --dataset tiny_shakespeare
```

### 4. Interactive Generation

```bash
python scripts/generate.py --prompt "To be or not to be" --temperature 0.8
```

### 5. Run Tests

```bash
pytest tests/ -v
```

---

## ğŸ“ Project Structure

```
parallax/
â”œâ”€â”€ parallax/                    # Main package
â”‚   â”œâ”€â”€ config.py                # Typed configuration (chex.dataclass)
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ attention.py         # MultiHeadAttention + RoPE + KV-Cache
â”‚   â”‚   â”œâ”€â”€ feedforward.py       # SwiGLU FFN
â”‚   â”‚   â”œâ”€â”€ normalization.py     # RMSNorm
â”‚   â”‚   â””â”€â”€ embeddings.py        # Weight-tied embeddings
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ block.py             # TransformerBlock (pre-norm residual)
â”‚   â”‚   â””â”€â”€ transformer.py       # TransformerLM (full decoder stack)
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ data.py              # Character tokenizer + dataset
â”‚   â”‚   â”œâ”€â”€ loss.py              # Cross-entropy + Z-loss
â”‚   â”‚   â”œâ”€â”€ optimizer.py         # AdamW + cosine schedule
â”‚   â”‚   â””â”€â”€ train_step.py        # JIT-compiled training step
â”‚   â”œâ”€â”€ distributed/
â”‚   â”‚   â””â”€â”€ pmap_trainer.py      # jax.pmap distributed training
â”‚   â””â”€â”€ inference/
â”‚       â””â”€â”€ generate.py          # Autoregressive generation
â”œâ”€â”€ tests/                       # 36 unit tests
â”‚   â”œâ”€â”€ test_layers.py           # Layer correctness tests
â”‚   â”œâ”€â”€ test_model.py            # Model integration tests
â”‚   â””â”€â”€ test_training.py         # Training + overfit tests
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Main training script
â”‚   â””â”€â”€ generate.py              # Interactive generation
â”œâ”€â”€ terraform/                   # GCP TPU infrastructure
â”‚   â”œâ”€â”€ main.tf                  # Provider configuration
â”‚   â”œâ”€â”€ gcp.tf                   # TPU VM, GCS, VPC resources
â”‚   â”œâ”€â”€ variables.tf             # Input variables
â”‚   â””â”€â”€ outputs.tf               # Access information
â””â”€â”€ .github/workflows/           # CI/CD
    â””â”€â”€ ci.yml                   # Lint, test, type check
```

---

## ğŸ§ª Testing Strategy

### Unit Tests (36 tests)

| Category | Tests | Purpose |
|----------|-------|---------|
| Layers | 14 | Numerical correctness of RMSNorm, RoPE, Attention, FFN |
| Model | 8 | Forward pass shapes, param counting, KV-cache consistency |
| Training | 14 | Loss functions, optimizer schedules, **overfit test** |

### The Overfit Test

This is the **critical correctness check** for any new model:

```python
def test_overfit_single_batch():
    """Train on 'AB AB AB' until loss â†’ 0 and accuracy â†’ 100%"""
    # If this fails, there's a bug in the implementation
```

### Run All Tests

```bash
# All tests
pytest tests/ -v

# Specific test file
pytest tests/test_layers.py -v

# With coverage
pytest tests/ --cov=parallax
```

---

## ğŸŒ Distributed Training

### jax.pmap (Data Parallelism)

```python
# Batch is sharded across devices
# Each device computes local gradients
# Gradients are averaged with pmean before update

@jax.pmap
def train_step(state, batch, rng):
    grads = compute_gradients(state.params, batch)
    grads = jax.lax.pmean(grads, axis_name='devices')  # All-reduce
    return state.apply_gradients(grads=grads)
```

### RNG Folding

Each device gets unique random dropout masks:

```python
device_id = jax.lax.axis_index('devices')
rng = jax.random.fold_in(rng, device_id)  # Unique per device
```

### Scaling Efficiency

| Devices | Expected Speedup | Actual (typical) |
|---------|------------------|------------------|
| 1 | 1.0x | 1.0x |
| 4 | 4.0x | ~3.8x |
| 8 | 8.0x | ~7.6x |

Communication overhead causes ~5% efficiency loss.

---

## â˜ï¸ Infrastructure (Terraform)

Deploy TPU training infrastructure to GCP:

### Quick Deploy

```bash
cd terraform

# Initialize
terraform init

# Configure (copy example)
cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars with your project_id

# Preview
terraform plan

# Deploy
terraform apply

# Connect
gcloud compute tpus tpu-vm ssh parallax-dev-tpu --zone=us-central1-a

# Destroy when done!
terraform destroy
```

### Resources Created

| Resource | Type | Purpose |
|----------|------|---------|
| TPU VM | v3-8 | 8 TPU v3 cores for training |
| GCS Bucket | Standard | Checkpoints and data storage |
| VPC Network | Custom | Isolated network |
| Service Account | IAM | Minimal permissions |

### âš ï¸ Cost Warning

- **TPU v3-8**: ~$8/hour (on-demand) or ~$2.40/hour (preemptible)
- **Always run `terraform destroy` when not training!**

---

## ğŸ”„ CI/CD Pipeline

GitHub Actions runs on every push:

```yaml
jobs:
  lint:     # Ruff linter + formatter
  test:     # pytest on Python 3.10, 3.11, 3.12
  typecheck: # Pyright (advisory)
```

---

## ğŸ“Š Configuration Presets

```python
from parallax.config import TINY_CONFIG, SMALL_CONFIG, BASE_CONFIG

# TINY: For quick experiments and tests
# - 128 hidden, 4 layers, 4 heads
# - ~400K parameters
# - Trains in seconds on CPU

# SMALL: For real training
# - 512 hidden, 6 layers, 8 heads
# - ~25M parameters
# - Trains in minutes on GPU

# BASE: Production-like
# - 768 hidden, 12 layers, 12 heads
# - ~125M parameters
# - Requires GPU/TPU
```

---

## ğŸ“ Research Engineering Practices

This codebase demonstrates key practices:

1. **Parametric Typing**: `chex.dataclass` for immutable, validated configs
2. **Pure Functions**: Stateless, JIT-compiled `train_step`
3. **Observability**: Logging gradient norm, parameter norm, Z-loss
4. **The Overfit Test**: Mathematical correctness validation
5. **Numerical Stability**: Float32 for norms, Z-loss for logits
6. **Modern Techniques**: RMSNorm, RoPE, SwiGLU, KV-cache

---

## ğŸ“š References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer
- [RoFormer: RoPE](https://arxiv.org/abs/2104.09864) - Rotary Positional Embeddings
- [GLU Variants](https://arxiv.org/abs/2002.05202) - SwiGLU activation
- [PaLM](https://arxiv.org/abs/2204.02311) - Z-loss technique
- [Llama](https://arxiv.org/abs/2302.13971) - RMSNorm + RoPE + SwiGLU combination

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [Llama](https://github.com/facebookresearch/llama) - Architecture inspiration
- [minGPT](https://github.com/karpathy/minGPT) - Educational approach
- [Flax](https://github.com/google/flax) - JAX neural network library
- [Optax](https://github.com/deepmind/optax) - Optimization library
